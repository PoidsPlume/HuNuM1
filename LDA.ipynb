{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca678142-5397-4c7a-bd6c-74a878f90544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim_models\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from mistralai import Mistral\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3962aeaf-9ace-4a62-a3a5-d9cec4a8a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f1568e-9913-4cb3-af3d-8176291d46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop = [\"janvier\", \"février\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\", \"août\", \"aout\", \"septembre\", \"octobre\", \"novembre\", \"décembre\",\n",
    "           \"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\", \"ça\", \"c'\", \"ce\", \"cela\", \"ceci\", \"moi\", \"toi\", \"lui\", \"sien\", \"leur\", \n",
    "          \"faire\", \"être\", \"avoir\", \"mettre\", \"aller\", \"devoir\", \"vouloir\", \"falloir\", \"devenir\", \"luire\", \"pouvoir\", \"savoir\", \"demeurer\", \"sembler\",\n",
    "           \"passer\", \"voir\", \"venir\", \"prendre\", \"trouver\", \"raconter\", \"déclarer\",\n",
    "           \"expliquer\", \"dire\", \"annoncer\", \"confirmer\", \n",
    "          \"sont\", \"est\", \"a\", \"ont\", \"vont\", \"seraient\", \"seront\",\n",
    "          \"jusque\", \"mais\", \"et\", \"donc\", \"or\", \"ni\", \"car\", \"jusqu\", \"jusqu'\", \"bien\", \"mal\", \"presque\", \"jusqu’\",\n",
    "          \"an\", \"année\", \"jour\", \"mois\",\n",
    "          \"...\", \"non\", \"oui\", \"presscontact\",\n",
    "          \"lundi\", \"mardi\", \"mercredi\", \"jeudi\", \"vendredi\", \"samedi\", \"dimanche\", \"hier\", \"demain\", \"aujourd'hui\",\n",
    "          \"abonné\", \"m.\", \"mme\", \"ici\", \"-t\", \"là\", \"là-bas\", \"-là\", \"cas\", \"trop\",\n",
    "          \"premier\", \"deuxième\", \"troisième\", \"dernier\", \"prochain\",\n",
    "          \"toujours\", \"jamais\", \"parfois\",\n",
    "          \"abonné\", \"article\", \"réservé\", \"e.a.\", \"s.t.\"\n",
    "          \"grand\", \"petit\", \"gros\", \"large\",\n",
    "          \"afp\", \"belga\", \"the\", \"rien\", \"conseiller\", \"rédaction\", \"figaro\",\n",
    "          \"femme\", \"homme\",\n",
    "           \"évident\", \"judicieux\"\n",
    "          #\"prostitué\", \"prostituée\", \"prostitution\", \"prostituer\",\n",
    "          'antérieur', 'toi', 'lors', 'il', 'moindres', 'pres', 'chaque', 'avaient', 'vôtre', 'parfois', 'siens', 'quel', 'nos', 'restant', 'restent', 'toi-même', \"c'\", 'jusque', 'la', 'm’', 'lesquels', 'anterieures', 'pourrais', 'chez', 'en', 'vous-mêmes', 'huit', 'envers', 'celles-là', 'tout', 'suis', 'douzième', 'certes', 'apres', 'moins', 'cinquième', 'était', 'ouvert', 'etaient', 'reste', 'directe', 'nôtres', 'leurs', 'on', 'd’', 'doivent', 'anterieure', 'différents', 'hi', 'deux', 'etant', 'lès', 'avait', 'ha', 'dont', 'être', 'ma', 'dix-neuf', 'elle', 'toute', 'auquel', 'sept', 'desquelles', 'etc', 'ça', 'dans', 'alors', 'sera', 'parmi', 'nous-mêmes', 'plutôt', 'vous', 'auront', 'stop', 'certain', 'seules', 'basee', 'préalable', 'devant', 'anterieur', 'allaient', 'c’', 'quelconque', 'onze', 'excepté', 'font', 'façon', 'étais', 'le', 'delà', 'lesquelles', 'ceux', 'neuvième', 'vôtres', 'auxquels', 'où', 'votres', 'prealable', 'toujours', 'eux', 'malgre', 'doit', 'toi-meme', 'siennes', 'y', \"n'\", 'ait', 's’', 'ne', 'quatre', 'cinquantaine', 'pendant', 'vé', 'elles', 'celui-la', 'té', 'qui', 'ayant', 'aurait', 'avoir', 'cent', 'ce', 'nombreux', 'suit', 'seuls', 'quelle', 'sent', 'voilà', 'votre', 'spécifique', 'différent', 'tels', 'aupres', 'seule', 'fait', 'attendu', 'huitième', 'vos', 'dire', 'depuis', 'nouveau', 'soi-meme', 'suivant', 'à', 'seul', 'suffit', 'des', 'rend', 'pu', 'pas', 'comme', 'tant', 'eh', 'j’', 'moi-même', \"qu'\", 'deuxièmement', 'ai', 'eu', 'tente', 'i', 'ceux-ci', 'ta', 'également', 'étant', 'celles-la', 'chacun', \"s'\", 'vas', 'fais', 'enfin', 'leur', 'suivre', 'auxquelles', 'trente', 'quelles', 'ouverts', 'seraient', 'suivants', 'tienne', 'relative', 'celle', 'toutes', 'environ', 'meme', 'malgré', 'moi', 'souvent', 'merci', 'six', 'differente', 'quatre-vingt', 'importe', 'specifique', 'aux', 'tend', 'est', 'possible', 'celles', 'elle-meme', 'ceux-là', 'désormais', 'mien', 'trois', 'soixante', 'quatrièmement', 'seize', 'neanmoins', 'vu', 'da', 'autres', 'quinze', 'pouvait', 'egalement', 'n’', 'puis', 'dedans', 'surtout', 'une', 'divers', 'là', 'tiens', 'puisque', 'néanmoins', 'dix-huit', 'dehors', 'effet', 'lorsque', 'vais', 'peut', 'tu', 'comment', 'les', 'lui-meme', 'premier', 'selon', 'combien', 'cela', 'faisaient', 'revoilà', 'hui', 'déja', 'sans', 'certaines', 'tous', 'concernant', 'lequel', 'certains', 'te', 'un', 'je', 'etais', 'va', 'avant', 'quelque', 'hep', 'l’', 'compris', 'proche', 'pense', 'si', 'plusieurs', 'du', 'différentes', 'me', 'au', 'cinquante', 'autrement', 'plus', 'durant', 'onzième', 'cet', 'cinq', 'car', 'memes', 'mon', 'nombreuses', 'parle', 'sous', 'première', 'soi', 'semblable', 'ces', 'sien', 'vers', 'hem', 'derrière', 'donc', 'ni', 'sienne', 'debout', 'ainsi', 'celle-ci', 'eux-mêmes', 'semblaient', 'es', 'soi-même', 'quelques', 'procedant', 'tellement', 'telles', 'mes', \"d'\", 'pour', 'tenir', 'treize', 'plutot', 'seront', 'cinquantième', 'certaine', 'faisant', 'elles-memes', 'celui-ci', 'partant', 'uns', 'celles-ci', 'antérieure', 'suivante', \"t'\", 'laisser', 'dit', 'mienne', 'ouias', 'chacune', 'ès', 'dits', 'unes', 'ci', 'derriere', 'hou', 'retour', 'spécifiques', 'laquelle', 'quand', 'tes', 'tres', 'houp', 'directement', 'voila', 'mille', 'miennes', 'septième', 'bat', 'différente', 'assez', 'autrui', 'cependant', 'antérieures', 'pourrait', 'elles-mêmes', 'devra', 'quiconque', 'voici', 'feront', 'douze', 'exactement', 'aussi', 'ceci', 'gens', 'et', 'longtemps', 'ses', 'etre', 'different', 'hé', 'aura', 'dessus', 'devers', 'quatorze', \"j'\", 'allons', 'soit', 'aie', \"l'\", 'hormis', 'premièrement', 'rendre', 'vingt', 'sait', 'quatrième', 'diverse', 'tel', 'dite', 'touchant', 'étaient', 'ont', 'ouste', 'desormais', 'quant-à-soi', 'semble', 'nous', 'peu', 'via', 'diverses', 'dessous', 'se', 'mêmes', 'sinon', 'telle', 'dix-sept', 'quoi', 'son', 'troisièmement', 'semblent', 'nul', 'déjà', 'bas', 'hue', 'peuvent', 'lui-même', 'qu’', 'differentes', 'mais', 'celle-la', 'ils', 'â', 'quarante', 'abord', 'près', 'a', 'pourquoi', 'seulement', 'moi-meme', 'celui-là', 'ouverte', 'desquels', 'dixième', 'dix', 'sur', 'auraient', 'duquel', 'même', 'nôtre', 'suffisante', 'dejà', 'dès', 'relativement', 'avais', 't’', 'deja', 'maint', 'personne', 'peux', 'ho', 'sixième', 'specifiques', 'suivantes', 'que', 'avons', 'entre', 'o', 'tiennes', 'ton', 'differents', 'elle-même', 'as', 'permet', 'parlent', 'precisement', 'outre', 'lui', 'maintenant', 'quels', \"m'\", 'ah', 'serait', 'autre', 'quant', 'tien', 'cette', 'sa', 'précisement', 'celle-là', 'vont', 'possibles', 'afin', 'troisième', 'miens', 'encore', 'ou', 'sont', \"quelqu'un\", 'etait', 'juste', 'deuxième', 'de', 'parler', 'sauf', 'revoila', 'na', 'notamment', 'or', 'très', 'facon', 'ô', 'avec', 'notre', 'revoici', 'par', 'après', 'tenant', 'jusqu', 'parce', 'hors', 'suffisant', 'quoique', 'celui']\n",
    "\n",
    "verbes_d_etat = ['accorder', 'affecter', 'afficher', 'allouer', 'apparaître', 'appartenir', 'assigner', 'attester', 'attribuer', 'avoir', 'choisir', 'communiquer', 'comprendre', 'compter', 'concerner', 'concéder', 'confier', 'consacrer', 'consister', 'constituer', 'contenir', 'demeurer', 'destiner', 'devenir', 'diffuser', 'dispatcher', 'disposer', 'distribuer', 'décider', 'dédier', 'démontrer', 'désigner', 'déterminer', 'dévoiler', 'exister', 'exposer', 'exprimer', 'fixer', 'impliquer', 'imprimer', 'inclure', 'indiquer', 'installer', 'introduire', 'localiser', 'manifester', 'montrer', 'nommer', 'octroyer', 'opter', 'paraître', 'partager', 'placer', 'positionner', 'posséder', 'prouver', 'préférer', 'présenter', 'publier', 'regarder', 'remettre', 'renfermer', 'représenter', 'rester', 'répartir', 'réserver', 'révéler', 'sembler', 'signifier', 'situer', 'symboliser', 'sélectionner', 'toucher', 'transmettre', 'témoigner', 'valoir', 'vouer', 'éditer', 'élire', 'établir', 'être']\n",
    "\n",
    "my_stop.extend(verbes_d_etat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4457386-6db1-4e91-a742-814e531483db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    motifs = [r\"\\b(\\d{1,2}h(?:\\d{2})?)\\b\", #heures\n",
    "              r\"\\b(\\d{1,2} heures)\\b\", #heures toutes lettres\n",
    "              r\"\\b(\\d{2}/\\d{2}/\\d{2,4}|\\d{2} (janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) \\d{4})\\b\" #dates\n",
    "              r\"\\d\"\n",
    "             ]\n",
    "    for motif in motifs:\n",
    "        text = re.sub(motif, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0104d31-4d78-4fd6-9566-8907cde35574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_txt(directory):\n",
    "    txt_files = os.listdir(directory)\n",
    "    articles = []\n",
    "    journaux = []\n",
    "    years = []\n",
    "    months = []\n",
    "    for txt in txt_files:\n",
    "        try:\n",
    "            with open(os.path.join(directory, txt), 'r', encoding='utf-8') as f:\n",
    "                articles.append(clean_text(f.read()))\n",
    "            cat, pol, auth, year, month, name = txt.split('_')\n",
    "            journaux.append(auth)\n",
    "            years.append(year)\n",
    "            months.append(month)\n",
    "        except OSError:\n",
    "            print(colored(f\"Erreur lors de l'ouverture/lecture du fichier {txt}.\", 'red'))\n",
    "    d = {'texte': articles, 'journal': journaux, 'année':years, 'mois':months}\n",
    "    return pd.DataFrame(data = d, columns = ['texte', 'journal', 'année', 'mois'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90e541db-3299-4cbd-978f-9409a20bda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df, my_stop):\n",
    "    txt = df['texte']\n",
    "    docs = []\n",
    "    for doc in tqdm(nlp.pipe(txt), total=len(txt)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.orth_ > 2 and not token.lemma_.lower() in my_stop: # supprime les mots \n",
    "                tokens.append( token.lemma_.lower() )\n",
    "        docs.append(tokens)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f3e457-9493-4912-b09a-daebfd40eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigram(tokens):\n",
    "    bigram = Phrases(tokens, min_count=5)\n",
    "\n",
    "    for index in range(len(tokens)):\n",
    "        for token in bigram[tokens[index]]:\n",
    "            if '_' in token:  # les bigrammes peuvent être reconnus par \"_\" qui concatène les mots\n",
    "                tokens[index].append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d8529c-ce5e-433e-890f-6dc187362329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2bow(bigrams):\n",
    "    dictionary = Dictionary(bigrams)\n",
    "    print('Nombre de mots unique dans les documents initiaux :', len(dictionary))\n",
    "\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.90)\n",
    "    print('Nombre de mots unique dans les documents après avoir enlevé les mots fréquents/peu fréquents :', len(dictionary))\n",
    "\n",
    "    #print(\"Exemple :\", dictionary.doc2bow(bigrams[4]))\n",
    "    corpus = [ dictionary.doc2bow(doc) for doc in bigrams]\n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c3a182-4cad-4553-95a6-f372623edf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(year:str):\n",
    "    df = pd.DataFrame(columns = ['texte', 'journal', 'année', 'mois'])\n",
    "    path = 'Sources/Europresse/txt/'\n",
    "    for month in range(1, 13):\n",
    "        month = str(month) if month > 9 else '0' + str(month)\n",
    "        folder = path + month + '_' + year\n",
    "        df_temp = import_txt(folder)\n",
    "        df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d556196f-079d-44f2-9e7b-3af25210cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param(year:str, my_stop):\n",
    "    print(\"Génération du dataframe\")\n",
    "    df = get_df(year)\n",
    "    print(\"Tokenization du dataframe\")\n",
    "    tokens = tokenize_df(df, my_stop)\n",
    "    print(\"Etablissement des bigrams\")\n",
    "    bigrams = find_bigram(tokens)\n",
    "    print(\"Constitution du corpus et du dictionnaire\")\n",
    "    corpus, dictionary = doc2bow(bigrams)\n",
    "    return df, tokens, bigrams, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d985a12-487a-4d09-a6d3-0feab929b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visu(corpus, dictionary, year:str):\n",
    "    model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=14, chunksize=1000, passes=10, random_state=26112002)\n",
    "    vis_data = pyLDAvis.gensim_models.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "    pyLDAvis.save_html(vis_data, 'html/LDA_' + year + '.html')\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a29a426-3ed9-4274-8213-e0c9a517a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_client():\n",
    "    api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "    model = \"mistral-large-latest\"\n",
    "\n",
    "    client = Mistral(api_key=api_key)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ae6704-14c4-4484-8a33-d676fa0591ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_name(words:str, client):\n",
    "    chat_response = client.chat.complete(\n",
    "    model= \"mistral-large-latest\",\n",
    "    messages = [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"J'essaie de déterminer les sujets traités par des articles de journaux. Je vais te donner 20 mots qui sont représentatifs du sujet. Donne moi le thème abordé par les journaux en un ou deux groupes nominaux.\n",
    "\n",
    "                            Les mots: {words}\n",
    "\n",
    "                            Ne donne que le thème, n'ajoute rien d'autre\"\"\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed7451be-a282-407e-86b6-f50cf9b43e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(words_topic, client):\n",
    "    topics = []\n",
    "    for i, words in enumerate(words_topic):\n",
    "        try:\n",
    "            topics.append(get_topic_name(words, client))\n",
    "            time.sleep(2)\n",
    "        except SDKError:\n",
    "            print(f\"Erreur lors de la communication avec le LLM lors du traitement du {i}e sujet\" ) \n",
    "    return topics   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "066328ca-d5f8-41dc-bf16-f402f93a1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_full(model):\n",
    "    words_topic = []\n",
    "    for (topic, words) in model.print_topics(num_words=20):\n",
    "        words = re.sub(r\" \\+ \", ', ', words)\n",
    "        words = re.sub(r\"\\d.\\d{3}\\*\", \"\", words)\n",
    "        words_topic.append(words)\n",
    "    client = init_client()\n",
    "    topics = find_topics(words_topic, client)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6809cd28-d8ae-4b35-9f74-ae391fc7dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération du dataframe\n",
      "Tokenization du dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7800/7800 [11:01<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etablissement des bigrams\n",
      "Constitution du corpus et du dictionnaire\n",
      "Nombre de mots unique dans les documents initiaux : 128738\n",
      "Nombre de mots unique dans les documents après avoir enlevé les mots fréquents/peu fréquents : 70319\n"
     ]
    }
   ],
   "source": [
    "df20, tokens20, bigrams20, corpus20, dictionary20 = get_param('2020', my_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7387e844-f93e-422c-b569-8bca4b3e05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model20 = get_visu(corpus20, dictionary20, '2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b529702-3018-45fd-94e4-3688074d31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics20 = get_topics_full(model20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35c95839-4bc5-4ef5-861a-0bb34ea498fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Programmation de films, cinéma',\n",
       " 'Législation sur la prostitution, Politique de santé publique',\n",
       " 'Mouvement féministe, Politique',\n",
       " 'Conflits militaires, islamisme',\n",
       " 'Industrie du divertissement, cinéma et musique',\n",
       " '\\n\\nProgrammestélévisés, séries et téléfilms',\n",
       " 'Lutte contre la prostitution des jeunes',\n",
       " 'Prostitution de mineurs, enquête judiciaire',\n",
       " 'Littérature, histoire',\n",
       " 'Musique et chanteurs célèbres',\n",
       " 'Affaire criminelle, enquête policière',\n",
       " 'Le procès de Jeffrey Epstein et Ghislaine Maxwell.',\n",
       " 'Vie urbaine à Paris',\n",
       " 'Cinéma et vie familiale']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c9862e6-7c0b-4d02-a6db-626c262270f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération du dataframe\n",
      "Tokenization du dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7519/7519 [08:39<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etablissement des bigrams\n",
      "Constitution du corpus et du dictionnaire\n",
      "Nombre de mots unique dans les documents initiaux : 135171\n",
      "Nombre de mots unique dans les documents après avoir enlevé les mots fréquents/peu fréquents : 75009\n"
     ]
    }
   ],
   "source": [
    "df21, tokens21, bigrams21, corpus21, dictionary21 = get_param('2021', my_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78a66585-9db6-4768-a7f6-020158b8058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model21 = get_visu(corpus21, dictionary21, '2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ce4effe-0930-4d65-932e-82427d7c5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics21 = get_topics_full(model21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6241a204-07cc-45f4-993b-610a7d4b3210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Procès judiciaire, politique nationale',\n",
       " 'Lutte contre la prostitution, aide sociale aux jeunes',\n",
       " 'Violence sexuelle contre les mineurs',\n",
       " 'Urbanisme et sécurité publique',\n",
       " 'Affaire Jeffrey Epstein, Ghislaine Maxwell',\n",
       " 'Prostitution et proxénétisme',\n",
       " 'Littérature, arts de la scène',\n",
       " 'Justice parlementaire à Bruxelles',\n",
       " 'Cinéma, notamment le film \"Le Silence des agneaux\" et son actrice Jodie Foster.',\n",
       " 'Militantisme féministe, droits sociaux',\n",
       " 'Relations internationales en Asie',\n",
       " 'Procès de Valérie Bacot, Affaire de justice',\n",
       " \"L'industrie du cinéma et des séries télévisées\",\n",
       " 'Vie de famille']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb7830fd-3000-4769-a802-2f4f205fdb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération du dataframe\n",
      "Tokenization du dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7906/7906 [09:33<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etablissement des bigrams\n",
      "Constitution du corpus et du dictionnaire\n",
      "Nombre de mots unique dans les documents initiaux : 147398\n",
      "Nombre de mots unique dans les documents après avoir enlevé les mots fréquents/peu fréquents : 78496\n"
     ]
    }
   ],
   "source": [
    "df22, tokens22, bigrams22, corpus22, dictionary22 = get_param('2022', my_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2ab8767-0f49-44d7-948c-b68adbc2f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model22 = get_visu(corpus22, dictionary22, '2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b59f1e2-9a74-47ba-936f-b6833218d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics22 = get_topics_full(model22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daa95c04-2509-44b2-8e18-e461be8b5232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Événements culturels, spectacles de théâtre',\n",
       " 'Séries télévisées, diffusion',\n",
       " 'Conflits internationaux, élections politiques',\n",
       " 'Prostitution et proxénétisme, enquêtes judiciaires.',\n",
       " 'Traite des êtres humains, Violence sexuelle sur mineurs',\n",
       " 'Cinéma et festivals de films',\n",
       " 'Cinéma',\n",
       " 'procès judiciaire, système judiciaire français',\n",
       " 'Variole du singe, vaccination',\n",
       " 'Prostitution, violence sexuelle',\n",
       " 'Exploitation sexuelle, accusations criminelles',\n",
       " 'Littérature et histoire',\n",
       " 'Arts et patrimoine culturel',\n",
       " 'Problèmes sociaux en milieu urbain']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cef2f9a-a872-424d-8896-2832727094d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération du dataframe\n",
      "Tokenization du dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7046/7046 [09:46<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etablissement des bigrams\n",
      "Constitution du corpus et du dictionnaire\n",
      "Nombre de mots unique dans les documents initiaux : 126524\n",
      "Nombre de mots unique dans les documents après avoir enlevé les mots fréquents/peu fréquents : 67760\n"
     ]
    }
   ],
   "source": [
    "df23, tokens23, bigrams23, corpus23, dictionary23 = get_param('2023', my_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3982295f-7680-457d-a439-fb25f7350e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model23 = get_visu(corpus23, dictionary23, '2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da79b149-540c-4823-99dd-2ad5c5bf5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics23 = get_topics_full(model23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd4e7a24-e16d-4800-a167-bf012c918afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Johnny Hallyday, Laeticia Hallyday',\n",
       " 'Actualités régionales',\n",
       " 'Affaire politique américaine, Relations internationales États-Unis',\n",
       " 'Industrie cinématographique et télévisuelle',\n",
       " 'Littérature et arts',\n",
       " 'Le trafic de drogue, les gangs criminels',\n",
       " 'Procès judiciaire de Silvio Berlusconi',\n",
       " 'Violences sexuelles',\n",
       " 'Prostitution et proxénétisme',\n",
       " 'Lutte contre la prostitution des jeunes de la rue',\n",
       " 'Patrimoine urbain, événements historiques',\n",
       " 'Famille, vie quotidienne',\n",
       " 'Prostitution urbaine, politique municipale',\n",
       " 'Arts et culture']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2bcd309-7c31-4978-ad60-8a3bfbe82e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération du dataframe\n",
      "Tokenization du dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7106/7106 [08:37<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etablissement des bigrams\n",
      "Constitution du corpus et du dictionnaire\n",
      "Nombre de mots unique dans les documents initiaux : 113005\n",
      "Nombre de mots unique dans les documents après avoir enlevé les mots fréquents/peu fréquents : 63771\n"
     ]
    }
   ],
   "source": [
    "df24, tokens24, bigrams24, corpus24, dictionary24 = get_param('2024', my_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "692dc132-7e29-4255-bdce-b339cdc2a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model24 = get_visu(corpus24, dictionary24, '2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83a5e93d-718f-446d-89b4-b464c1d00fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics24 = get_topics_full(model24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1997ff3e-c101-443d-92c5-6a8cc488273d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Procès pour agression sexuelle',\n",
       " 'Vie urbaine',\n",
       " 'Littérature, roman',\n",
       " 'La prostitution des mineurs',\n",
       " 'Trafic de drogue international, réseaux criminels.',\n",
       " 'Programmes télévisés et cinéma.',\n",
       " 'Arts et culture',\n",
       " 'Violence criminelle et enquêtes judiciaires',\n",
       " 'La Révolution française, la politique républicaine',\n",
       " 'Violence sexuelle, Abus familial',\n",
       " 'Service client, gestion des questions',\n",
       " 'Enquête sur Donald Trump et Matt Gaetz',\n",
       " 'Cinéma, festival de Cannes',\n",
       " 'Prostitution de mineurs, réseaux de proxénétisme']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48a5c97-a85b-43ca-8f4e-dc34ea07b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"] = \"wnH0JnTK4lSoh8Q2GHTe4PsmMrYeMjIW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ae19ea5-c666-4cb7-a514-a2d625e692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03d30dac-bd4e-4972-8d8e-c21911f212ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Sources/Europresse/pyData/tokens_LDA_2020.bin\", \"wb\") as f:\n",
    "    pickle.dump(tokens20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae846536-8121-4b94-bd75-758e007befec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Sources/Europresse/pyData/tokens_LDA_2020.bin\", \"rb\") as f:\n",
    "    essai_toks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0772a8bc-82cc-4a91-bb91-55869dd8b175",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'orth_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_420372/2994651721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0messai_toks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'orth_'"
     ]
    }
   ],
   "source": [
    "essai_toks.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b047e-9f63-48ad-8556-401b0addfece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
